%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}

\title{\LARGE \bf
One Eye Leads the Blind: Asymmetric Multi-Robot Navigation with Visual Guidance
}


\author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
        University of Twente, 7500 AE Enschede, The Netherlands
        {\tt\small albert.author@papercept.net}}%
\thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In field operations where robots are exposed to extreme environments, the risk of sensor failure is a reality that affects the performance of robots and the ultimate success of their operation. This paper investigates the challenge of Asymmetric Multi-Robot Navigation, addressing a scenario where a sensing-rich "Leader" robot must guide a sensor-less "Follower" robot through an unknown environment.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Unmanned robotic systems are progressively being deployed in hazardous 
environments, including reconnaissance, search and rescue, and 
disaster response, where human presence is dangerous or not feasible~\cite{wang2025multi}. 
However, these missions often occur under harsh conditions that accelerate sensor degradation: dust and debris occlude cameras, electromagnetic interference disrupts lidar, and physical impacts could damage the perception hardware~\cite{ebadi2022present}. When a robot's sensors fail mid-mission in remote or inaccessible locations (e.g., underground tunnels, disaster sites, or enemy territory), the most likely response is to abort the mission and abandon the robot, resulting in costly equipment loss and incomplete objectives. This raises the question of whether a functional teammate can actively guide a sensor-impaired robot to safety or mission completion rather than abandoning it.

Asymmetric coordination strategies in multi-robot systems have been proposed to address mismatched capabilities among teammates. One approach uses a fixed sensor infrastructure that performs perception and localization on the robots' behalf, enabling effective navigation and coordination despite limited onboard sensing. This approach may not work in situations where setting up a fixed sensor infrastructure in the environment is not possible. For example, in dangerous environments that are being explored for the first time, it may be infeasible to set up such infrastructure.

Another approach is the leader–follower paradigm, in which a sensor-rich leader with privileged information computes safe trajectories while follower robots track and maintain formation along the leader’s path~\cite{petrinic2013leader}. However, this approach typically assumes that followers retain at least minimal sensing~\cite{wang2019vision} to track the leader or localize within the environment, an assumption that breaks down when a follower’s sensors have completely failed. This gap motivates the need for recovery mechanisms that remain effective even when a robot's perception sensor has failed.

Importantly, asymmetric navigation is not unique to robotic systems. For example, in complex, crowded, or unfamiliar environments, a blind person is often guided by a sighted companion who has privileged information about the environment, plans safe navigation trajectories, and communicates them through physical contact and voice, allowing safe movement while relying entirely on the guide. A similar idea appears in human–robot interaction, where robotic guide dogs with visual perception assist blind humans~\cite{hwang2025guidenav}. Inspired by these examples, we ask if a visually capable robot can guide another robot that has no perception at all, enabling safe navigation purely through its own sensing, localization and motion-based communication.

To address the challenges of asymmetric navigation under partial observability, we adopt a two-phase reinforcement learning framework based on privileged knowledge distillation. In Phase 1, we train a teacher policy in simulation using deep reinforcement learning (RL) with access to full ground-truth environment states, enabling the policy to learn robust navigation strategies with complete situational awareness. In Phase 2, we distill this expertise into a student policy that operates using only onboard sensing and limited observations. The student is trained via supervised imitation to replicate the teacher’s actions, thereby transferring privileged knowledge without requiring full state access at deployment. This simulation-to-reality pipeline enables strong generalization to unseen environments. 

We validate our approach through real-world deployment on two Hiwonder Orin robots, where a vision-less follower successfully navigates cluttered environments by executing movement commands communicated by a privileged leader.

The key contributions of this paper are as follows:

\begin{itemize}
    \item We introduce a novel asymmetric navigation framework in which a vision-enabled robot guides a vision-less robot through cluttered environments via communicated motion commands.
    
    \item We demonstrate successful real-world generalization through deployment on physical robots, validating robust navigation performance across diverse cluttered environments.

    
\end{itemize}


\section{RELATED WORKS}

\subsection{Navigation with External Sensing}

Recent work has demonstrated that autonomous navigation does not strictly require onboard exteroceptive sensors when external intelligence is available. Verma and Savkin~\cite{verma2025navigation} present a dynamic programming-based algorithm that enables a sensorless mobile robot to navigate dynamic environments using a fixed external sensor network. Their approach offloads localization, obstacle tracking, and path planning entirely to fixed LiDAR infrastructure, proving that a robot can react to dynamic obstacles without any onboard perception. In a related line of work, Savkin and Huang~\cite{savkin2020robot} propose safe robot navigation among moving and steady obstacles using a network of distributed sensors in smart city environments, further demonstrating the viability of infrastructure-based perception. However, both paradigms rely on pre-installed environmental sensors, which is infeasible in previously unexplored or hazardous settings where no such infrastructure can be established.

\subsection{Privileged Learning and Knowledge Distillation}

The methodology of transferring navigational competency from a knowledgeable agent to a sensor-limited agent has been explored through teacher--student and teach-and-repeat paradigms. Lee et al.~\cite{lee2020learning} introduced a foundational privileged-learning framework for quadrupedal locomotion, in which a teacher policy trained with noiseless ground-truth states transfers its knowledge to a student that relies solely on noisy proprioceptive feedback. This paradigm has since been extended across multiple domains. Qu et al.~\cite{qu2024versatile} utilize a similar two-stage process for low-cost hexapod robots, distilling a teacher trained with height maps and joint feedback into a student that depends only on limited onboard sensors. He et al.~\cite{he2024agile} propose the Agile But Safe (ABS) framework, employing a dual-policy architecture (agile vs.\ recovery) governed by reach-avoid values to enable both aggressive and safety-critical locomotion. Hoeller et al.~\cite{hoeller2021learning} demonstrate that learning a belief-state representation from depth images enables a robot to handle partial observability and dynamic obstacles, a capability that is essential for a leader robot tasked with anticipating hazards on behalf of a sensorless follower. To bridge the sim-to-real gap, Yu et al.~\cite{yu2025depth} introduce domain adaptation techniques that align simulated depth features with real-world inputs, ensuring that learned perception remains reliable during physical deployment. While these approaches reduce the sensor requirements on individual robots, they still assume that each agent retains at least some residual onboard sensing. None address the extreme case where a follower robot has \emph{no} perception capability whatsoever.

\subsection{Multi-Robot Coordination and Heterogeneous Teams}

Multi-robot navigation under decentralized, partially observable conditions has been widely studied. Chen et al.~\cite{chen2017decentralized} demonstrate that deep reinforcement learning can learn cooperative collision-avoidance policies for multi-agent systems, enabling each robot to make navigation decisions using only its local observations. Bettini et al.~\cite{bettini2023heterogeneous} introduce HetGPPO, a paradigm for training heterogeneous multi-agent reinforcement learning policies that exploits diversity in physical and sensory capabilities, showing that heterogeneity can improve both task performance and resilience to noise. In traditional multi-robot formation, behavior-based strategies~\cite{balch2002behavior} and collective transport frameworks~\cite{farivarnejad2022multirobot} coordinate teams through reactive controllers, but assume homogeneous or near-homogeneous sensing. Leader--follower formation control~\cite{petrinic2013leader, wang2019vision} has been widely investigated, yet existing approaches typically require the follower to retain at least minimal onboard sensing, such as vision-based tracking of the leader, to localize within the environment. The assumption that breaks down when perception hardware fails entirely.

\subsection{Assistive Guidance and Human--Robot Interaction}

The concept of asymmetric guidance is well established in human--robot interaction (HRI) for the visually impaired. Hwang et al.~\cite{hwang2025guidenav} developed GuideNav, a robotic guide-dog system that uses vision-based teach-and-repeat methods to guide blind humans over kilometer-scale routes without LiDAR. Their success suggests that vision-based topological maps can serve as a viable, low-bandwidth mechanism for linking a sighted guide with a vision-less agent. Complementarily, Xiao et al.~\cite{xiao2021robotic} present a robotic guide cane that combines obstacle detection with haptic feedback to assist visually impaired users in indoor environments, demonstrating that carefully designed communication channels can substitute for a user's absent perception. These HRI systems demonstrate the feasibility of asymmetric navigation but rely on a human in the loop; translating these principles to fully autonomous robot--robot teams remains an open challenge.

In contrast to prior work, our framework targets the most extreme form of asymmetry: a leader robot with full visual perception must guide a follower that possesses \emph{no} exteroceptive sensors at all, without relying on any fixed environmental infrastructure. Unlike infrastructure-based methods~\cite{verma2025navigation, savkin2020robot}, we require no pre-installed sensors. Unlike privileged-learning approaches~\cite{lee2020learning, qu2024versatile}, we do not assume that each agent retains residual onboard perception. And unlike HRI guidance systems~\cite{hwang2025guidenav, xiao2021robotic}, both agents are fully autonomous. We combine privileged knowledge distillation with an explicit inter-robot communication channel for motion commands, enabling the sensorless follower to navigate cluttered, unknown environments purely through the leader's situational awareness.


\section{PROBLEM FORMULATION}

Asymmetric multi-robot navigation, where a fully equipped leader must guide a sensorless follower through unknown, cluttered environments, presents fundamental challenges for conventional planning paradigms. Classical approaches would require exhaustive hand-engineering of reactive rules to anticipate the combinatorial diversity of obstacle configurations, robot-pair geometries, and communication-induced dynamics. Moreover, the leader must reason jointly about its own collision avoidance, the follower's safety, and the temporal coordination of communicated motion commands, all under partial observability. Deep RL offers a principled alternative: given a well-structured problem formulation, a DRL agent can discover effective coordination strategies directly from experience, without requiring explicit enumeration of environmental cases. We therefore adopt DRL and formulate the asymmetric navigation task as a partially observable Markov decision process (POMDP).

We model the problem as a POMDP defined by the tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \mathcal{O}, \mathcal{Z}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{T}$ is the state transition function, $\mathcal{R}$ is the reward function, $\mathcal{O}$ is the observation space, $\mathcal{Z}$ is the observation function, and $\gamma \in [0,1)$ is the discount factor. The objective is to learn a policy $\pi(a_t \mid o_{1:t})$ that maximizes the expected cumulative discounted reward $\mathbb{E}\left[\sum_{t=0}^{T} \gamma^t R(s_t, a_t)\right]$.

A POMDP formulation is necessary rather than a standard MDP because the leader robot does not have access to the complete environment state. Although the leader is equipped with onboard perception, its sensors provide only a local, noisy view of the surroundings. The true state, including unobserved obstacle regions, the follower's exact pose uncertainty, and future environmental dynamics, remains hidden. The leader must therefore act on partial observations and implicitly maintain a belief over the underlying state.


\section{Methodology}

\subsection{System Overview}

We adopt a two-phase privileged learning framework~\cite{lee2020learning} to train the leader's navigation policy for asymmetric multi-robot guidance. The central idea is to decouple the problem of \emph{what to do} from the problem of \emph{what can be observed}: a teacher policy first learns an expert coordination strategy with access to privileged ground-truth information available only in simulation, and a student policy subsequently learns to reproduce that strategy using only the sensor observations available during real-world deployment. This decomposition addresses two challenges simultaneously: (i) it simplifies policy optimization by allowing the teacher to train under full state observability, avoiding the compounding difficulties of partial observability and exploration; and (ii) it produces a deployable student that operates entirely within the observation constraints of the physical robot, bridging the simulation-to-reality gap without requiring domain randomization of the privileged state channels.

\subsubsection{Phase 1: Teacher Policy Training}

In the first phase, we train a teacher policy $\pi_T$ in simulation using deep reinforcement learning with access to the full environment state $s_t \in \mathcal{S}$. The privileged state encompasses ground-truth information that is not available to the physical robot's onboard sensors, including the exact positions and velocities of all obstacles, the precise relative pose between the leader and follower, the global map of the environment, and the follower's true kinematic state. Formally, the teacher receives the privileged observation $o_t^T = \phi_T(s_t)$, where $\phi_T$ is a deterministic mapping from the full state to the teacher's observation, and learns a policy $\pi_T(a_t \mid o_t^T)$ that maximizes the expected cumulative discounted reward:
%
\begin{equation}
    \pi_T^{*} = \arg\max_{\pi_T} \; \mathbb{E}_{\pi_T}\left[\sum_{t=0}^{T} \gamma^t R(s_t, a_t)\right].
    \label{eq:teacher_objective}
\end{equation}
%
Because the teacher observes the complete state, its training reduces to a fully observable RL problem, eliminating the need to learn implicit state estimation from noisy, partial sensor data. This allows the optimization to focus entirely on discovering effective coordination strategies, specifically how the leader should plan its own trajectory, anticipate obstacles on the follower's path, and time the communicated motion commands for safe and efficient joint navigation. The teacher is trained using Proximal Policy Optimization (PPO) in a parallelized simulation environment with randomized obstacle configurations, initial poses, and goal locations, ensuring that the resulting policy generalizes across diverse scenarios.

\subsubsection{Phase 2: Knowledge Distillation to Student Policy}

In the second phase, the trained teacher's knowledge is distilled into a student policy $\pi_S$ through supervised learning. Crucially, the student receives only the observation $o_t^S = \phi_S(s_t)$, where $\phi_S$ maps the state to the subset of information that is available from the leader's onboard sensors at deployment. This observation includes the leader's local sensor readings (e.g., depth or LiDAR scans within a limited field of view), the leader's own odometry, and the estimated relative position of the follower obtained through onboard sensing. The student explicitly lacks the privileged channels available to the teacher, such as exact obstacle positions beyond the sensor range, the global map layout, and the follower's true kinematic state.

The student is trained by rolling out the teacher policy in simulation, collecting paired datasets of student observations and teacher actions $\mathcal{D} = \{(o_t^S, a_t^T)\}_{t=1}^{N}$, where $a_t^T \sim \pi_T(a_t \mid o_t^T)$. The student policy is then optimized to minimize the behavioral divergence from the teacher:
%
\begin{equation}
    \pi_S^{*} = \arg\min_{\pi_S} \; \mathbb{E}_{(o^S, a^T) \sim \mathcal{D}}\left[\mathcal{L}\!\left(\pi_S(o^S),\; a^T\right)\right],
    \label{eq:student_objective}
\end{equation}
%
where $\mathcal{L}$ is a supervised loss function (e.g., mean squared error for continuous actions). Because the training data is generated by rolling out the expert teacher, the student learns to imitate the teacher's decisions while being conditioned exclusively on deployment-available observations. In effect, the distillation process compresses the teacher's privileged reasoning into a mapping from limited sensory input to expert actions.

\subsubsection{Information Asymmetry and Sim-to-Real Transfer}

The information asymmetry between the teacher and student is a deliberate design choice that underpins the framework's effectiveness for sim-to-real transfer. Table~\ref{tab:info_asymmetry} summarizes the observation channels available to each policy.

\begin{table}[h]
\centering
\caption{Information available to the teacher vs.\ student policy.}
\label{tab:info_asymmetry}
\begin{tabular}{l c c}
\hline
\textbf{Observation Channel} & \textbf{Teacher} & \textbf{Student} \\
\hline
Global obstacle positions & \checkmark & -- \\
Environment map / layout & \checkmark & -- \\
Follower true kinematic state & \checkmark & -- \\
Leader local sensor readings & \checkmark & \checkmark \\
Leader odometry & \checkmark & \checkmark \\
Estimated follower relative pose & \checkmark & \checkmark \\
\hline
\end{tabular}
\end{table}

The teacher's privileged access to the full ground-truth state allows it to learn a near-optimal coordination strategy unencumbered by sensor noise or occlusions. The student, by construction, observes only the modalities that will be physically available on the leader robot. This asymmetry ensures that no component of the deployed policy depends on simulation-specific information, and therefore the sim-to-real gap is confined to the fidelity of the sensor simulation (e.g., depth rendering, noise models) rather than the availability of entire observation channels. Because the student never receives privileged state during training, it is forced to extract the behaviorally relevant features from its limited observations, effectively learning an implicit belief representation that compensates for the missing information.

This two-phase pipeline offers several practical advantages. First, training the teacher with privileged information yields higher-quality expert demonstrations than would be achieved by training directly under partial observability, where the coupled challenges of exploration and state estimation significantly slow convergence. Second, the supervised distillation step is more sample-efficient and stable than end-to-end RL training of the student, since the learning signal comes from an already-converged expert rather than sparse environmental rewards. Third, the modular architecture allows the teacher and student to be developed and debugged independently: the teacher can be validated against ground-truth metrics in simulation, and the student can be evaluated by measuring action-prediction accuracy before any physical deployment.


\section{Experiments and Results}
\label{sec:experiments}

\section{Real-World Demonstration}

\section{Conclusion}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


\bibliographystyle{ieeetr}
\bibliography{references}






% \end{thebibliography}




\end{document}
