%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{titlesec}

\title{\LARGE \bf
One Eye Leads the Blind: Asymmetric Multi-Robot Navigation with Visual Guidance
}

\titleformat{\paragraph}[block]{\bfseries}{}{0pt}{}
\titlespacing*{\paragraph}{0pt}{1ex}{0.5ex}

% \author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small albert.author@papercept.net}}%
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{Anonymous Authors $^{1}$% <-this % stops a space
\thanks{*Support is anonymous}% <-this % stops a space
\thanks{$^{1}$Authors institutions and contact information are anonymous }%
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In field operations where robots are exposed to extreme environments, the risk of sensor failure is a reality that affects the performance of robots and the ultimate success of their operation. This paper investigates the challenge of Asymmetric Multi-Robot Navigation, addressing a scenario where a sensing-rich "Leader" robot must guide a sensor-less "Follower" robot through an unknown environment.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Unmanned robotic systems are progressively being deployed in hazardous 
environments, including reconnaissance, search and rescue, and 
disaster response, where human presence is dangerous or not feasible~\cite{wang2025multi}. 
However, these missions often occur under harsh conditions degrade sensors: dust and debris occlude cameras, electromagnetic interference disrupts lidar, and physical impacts can damage hardware~\cite{ebadi2022present}. When a robot's sensors fail mid-mission in remote or inaccessible locations, the most likely response is to abort the mission and abandon the robot.  This results in costly equipment loss and incomplete objectives. This raises the question of whether a functional teammate can actively guide a sensor-impaired robot to safety or mission completion rather than abandoning it.

Asymmetric coordination strategies in multi-robot systems have been proposed to address mismatched capabilities among teammates. One approach uses a fixed sensor infrastructure that performs perception and localization on the robots' behalf, enabling effective navigation and coordination despite limited onboard sensing. This approach may not work in situations where setting up a fixed sensor infrastructure in the environment is not possible. For example, in dangerous environments that are being explored for the first time, it may be infeasible to set up such infrastructure.

Another approach is the leader–follower paradigm, in which a sensor-rich leader with privileged information computes safe trajectories while follower robots track and maintain formation along the leader’s path~\cite{petrinic2013leader}. However, this approach typically assumes that followers retain at least minimal sensing~\cite{wang2019vision} to track the leader or localize within the environment, an assumption that breaks down when a follower’s sensors have completely failed. This gap motivates the need for recovery mechanisms that remain effective even when a robot's perception sensor has failed.

Importantly, asymmetric navigation is not unique to robotic systems. For example, in complex, crowded, or unfamiliar environments, a blind person is often guided by a sighted companion who has privileged information about the environment, plans safe navigation trajectories, and communicates them through physical contact and voice, allowing safe movement while relying entirely on the guide. A similar idea appears in human–robot interaction, where robotic guide dogs with visual perception assist blind humans~\cite{hwang2025guidenav}. Inspired by these examples, we ask if a visually capable robot can guide another robot that has no perception at all, enabling safe navigation purely through its own sensing, localization and motion-based communication.

To address the challenges of asymmetric navigation under partial observability, we adopt a two-phase reinforcement learning framework based on privileged knowledge distillation. In Phase 1, we train a teacher policy in simulation using deep reinforcement learning (RL) with access to full ground-truth environment states, enabling the policy to learn robust navigation strategies with complete situational awareness. In Phase 2, we distill this expertise into a student policy that operates using only onboard sensing and limited observations. The student is trained via supervised imitation to replicate the teacher’s actions, thereby transferring privileged knowledge without requiring full state access at deployment. This simulation-to-reality pipeline enables strong generalization to unseen environments. 

We validate our approach through real-world deployment on two Hiwonder Orin robots, where a vision-less follower successfully navigates cluttered environments by executing movement commands communicated by a privileged leader.

The key contributions of this paper are as follows:

\begin{itemize}
    \item We introduce a novel asymmetric navigation framework in which a vision-enabled robot guides a vision-less robot through cluttered environments via communicated motion commands.
    
    \item We demonstrate successful real-world generalization through deployment on physical robots, validating robust navigation performance across diverse cluttered environments.

    
\end{itemize}


\section{RELATED WORKS}

\subsection{Navigation with External Sensing}

Recent work has demonstrated that autonomous navigation does not strictly require onboard exteroceptive sensors when external intelligence is available. Verma and Savkin~\cite{verma2025navigation} present a dynamic programming-based algorithm that enables a sensorless mobile robot to navigate dynamic environments using a fixed external sensor network. Their approach offloads localization, obstacle tracking, and path planning entirely to fixed LiDAR infrastructure, proving that a robot can react to dynamic obstacles without any onboard perception. In a related line of work, Savkin and Huang~\cite{savkin2020robot} propose safe robot navigation among moving and steady obstacles using a network of distributed sensors in smart city environments, further demonstrating the viability of infrastructure-based perception. However, both paradigms rely on pre-installed environmental sensors, which is infeasible in previously unexplored or hazardous settings where no such infrastructure can be established.

\subsection{Privileged Learning and Knowledge Distillation}

The methodology of transferring navigational competency from a knowledgeable agent to a sensor-limited agent has been explored through teacher--student and teach-and-repeat paradigms. Lee et al.~\cite{lee2020learning} introduced a foundational privileged-learning framework for quadrupedal locomotion, in which a teacher policy trained with noiseless ground-truth states transfers its knowledge to a student that relies solely on noisy proprioceptive feedback. This paradigm has since been extended across multiple domains. Qu et al.~\cite{qu2024versatile} utilize a similar two-stage process for low-cost hexapod robots, distilling a teacher trained with height maps and joint feedback into a student that depends only on limited onboard sensors. He et al.~\cite{he2024agile} propose the Agile But Safe (ABS) framework, employing a dual-policy architecture (agile vs.\ recovery) governed by reach-avoid values to enable both aggressive and safety-critical locomotion. Hoeller et al.~\cite{hoeller2021learning} demonstrate that learning a belief-state representation from depth images enables a robot to handle partial observability and dynamic obstacles, a capability that is essential for a leader robot tasked with anticipating hazards on behalf of a sensorless follower. To bridge the sim-to-real gap, Yu et al.~\cite{yu2025depth} introduce domain adaptation techniques that align simulated depth features with real-world inputs, ensuring that learned perception remains reliable during physical deployment. While these approaches reduce the sensor requirements on individual robots, they still assume that each agent retains at least some residual onboard sensing. None address the extreme case where a follower robot has \emph{no} perception capability whatsoever.

\subsection{Multi-Robot Coordination and Heterogeneous Teams}

Multi-robot navigation under decentralized, partially observable conditions has been widely studied. Chen et al.~\cite{chen2017decentralized} demonstrate that deep reinforcement learning can learn cooperative collision-avoidance policies for multi-agent systems, enabling each robot to make navigation decisions using only its local observations. Bettini et al.~\cite{bettini2023heterogeneous} introduce HetGPPO, a paradigm for training heterogeneous multi-agent reinforcement learning policies that exploit diversity in physical and sensory capabilities, showing that heterogeneity can improve both task performance and resilience to noise. In traditional multi-robot formation, behavior-based strategies~\cite{balch2002behavior} and collective transport frameworks~\cite{farivarnejad2022multirobot} coordinate teams through reactive controllers, but they assume homogeneous or near-homogeneous sensing. Leader-follower formation control~\cite{petrinic2013leader, wang2019vision} has been widely investigated; however, existing approaches typically require the follower to retain at least minimal onboard sensing, such as vision-based tracking of the leader, to localize within the environment. This assumption breaks down when perception hardware fails entirely.

\subsection{Assistive Guidance and Human--Robot Interaction}

The concept of asymmetric guidance is well established in human-robot interaction (HRI) for the visually impaired. Hwang et al.~\cite{hwang2025guidenav} developed GuideNav, a robotic guide-dog system that uses vision-based teach-and-repeat methods to guide blind humans over kilometer-scale routes without LiDAR. Their success suggests that vision-based topological maps can serve as a viable, low-bandwidth mechanism for linking a sighted guide with a vision-less agent. Complementarily, Xiao et al.~\cite{xiao2021robotic} present a robotic guide cane that combines obstacle detection with haptic feedback to assist visually impaired users in indoor environments, demonstrating that carefully designed communication channels can substitute for a user's absent perception. These HRI systems demonstrate the feasibility of asymmetric navigation but rely on a human in the loop; translating these principles to fully autonomous robot-robot teams remains an open challenge.

In contrast to prior work, our framework targets the most extreme form of asymmetry: a leader robot with full visual perception must guide a follower that possesses \emph{no} exteroceptive sensors at all, without relying on any fixed environmental infrastructure. Unlike infrastructure-based methods~\cite{verma2025navigation, savkin2020robot}, we require no pre-installed sensors. Unlike privileged-learning approaches~\cite{lee2020learning, qu2024versatile}, we do not assume that each agent retains residual onboard perception. And unlike HRI guidance systems~\cite{hwang2025guidenav, xiao2021robotic}, both agents are fully autonomous. We combine privileged knowledge distillation with an explicit inter-robot communication channel for motion commands, enabling the sensorless follower to navigate cluttered, unknown environments purely through the leader's situational awareness.


\section{PROBLEM FORMULATION}

Asymmetric multi-robot navigation, where a fully equipped leader must guide a sensorless follower through unknown, cluttered environments, presents fundamental challenges for conventional planning paradigms. Classical approaches would require exhaustive hand-engineering of reactive rules to anticipate the combinatorial diversity of obstacle configurations, robot-pair geometries, and communication-induced dynamics. Moreover, the leader must reason jointly about its own collision avoidance, the follower's safety, and the temporal coordination of communicated motion commands, all under partial observability. Deep RL offers a principled alternative: given a well-structured problem formulation, a DRL agent can discover effective coordination strategies directly from experience, without requiring explicit enumeration of environmental cases. We therefore adopt DRL and formulate the asymmetric navigation task as a partially observable Markov decision process (POMDP).

We model the problem as a POMDP defined by the tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \mathcal{O}, \mathcal{Z}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{T}$ is the state transition function, $\mathcal{R}$ is the reward function, $\mathcal{O}$ is the observation space, $\mathcal{Z}$ is the observation function, and $\gamma \in [0,1)$ is the discount factor. The objective is to learn a policy $\pi(a_t \mid o_{1:t})$ that maximizes the expected cumulative discounted reward $\mathbb{E}\left[\sum_{t=0}^{T} \gamma^t R(s_t, a_t)\right]$.

A POMDP formulation is necessary rather than a standard MDP because the leader robot does not have access to the complete environment state. Although the leader is equipped with onboard perception, its sensors provide only a local, noisy view of the surroundings. The true state, including unobserved obstacle regions, the follower's exact pose uncertainty, and future environmental dynamics, remains hidden. The leader must therefore act on partial observations and implicitly maintain a belief over the underlying state.


\section{Methodology}

\subsection{System Overview}

We adopt a two-phase student-teacher learning framework~\cite{lee2020learning} to train the leader's navigation policy for asymmetric multi-robot guidance. The central idea is to decouple the problem of \emph{what to do} from the problem of \emph{what can be observed}: a teacher policy first learns an expert coordination strategy with access to privileged ground-truth information available only in simulation, and a student policy subsequently learns to reproduce that strategy using only the sensor observations available during real-world deployment. This decomposition addresses two challenges simultaneously: (i) it simplifies policy optimization by allowing the teacher to train under full state observability, avoiding the compounding difficulties of partial observability and exploration; and (ii) it produces a deployable student that operates entirely within the observation constraints of the physical robot, bridging the simulation-to-reality gap without requiring domain randomization of the privileged state channels.

\subsubsection{Phase 1: Teacher Policy Training}

In the first phase, we train a teacher policy $\pi_T$ end-to-end in simulation using deep RL with access to privileged information. Formally, the teacher receives the privileged observation $o_t^T = \phi_T(s_t)$, where $\phi_T$ is a deterministic mapping from the full state to the teacher's observation, and learns a policy $\pi_T(a_t \mid o_t^T)$ to achieve mapless navigation by maximizing the expected cumulative reward over each episode.

To facilitate efficient learning, we use raycaster information as the privileged information to train the teacher policy. The raycaster data captures the heights of the terrain and items near the robot. This privileged information allows for efficient training, as the raycasts serve as a lightweight proxy for depth images that are significantly cheaper to compute.

The reward function considers the various aspects of the cooperative navigation task. It consists of three primary components: (1) a progress reward designed to get the faulty agent to its goal location, which increases with the closing distance to the goal; (2) an active perception reward that encourages the healthy agent to look around, ensuring it can effectively guide the faulty agent around obstacles; and (3) safety penalties that discourage collision of the healthy agent or the faulty agent.


\subsubsection{Phase 2: Knowledge Distillation to Student Policy}

In the second phase of training, we use supervised learning to distill the expert behavior learned by the teacher into a student policy. This student policy utilizes an observation space designed to match the sensory inputs that will be available during physical deployment on the real robot. Specifically, the student operates on a stream of front-facing depth images combined with proprioceptive data state. Similar to prior work in legged locomotion with egocentric vision~\cite{agarwal2022legged}, we employ Dataset Aggregation (DAgger) to robustly train the student policy, iteratively minimizing the action discrepancy between the student's predictions and the teacher's expert actions.

\begin{table}[h]
\centering
\caption{Information available to the teacher vs.\ student policy.}
\label{tab:info_asymmetry}
\begin{tabular}{l c c}
\hline
\textbf{Observation Channel} & \textbf{Teacher} & \textbf{Student} \\
\hline
Global obstacle positions & \checkmark & -- \\
Environment map / layout & \checkmark & -- \\
Follower true kinematic state & \checkmark & -- \\
Leader local sensor readings & \checkmark & \checkmark \\
Leader odometry & \checkmark & \checkmark \\
Estimated follower relative pose & \checkmark & \checkmark \\
\hline
\end{tabular}
\end{table}


\section{Experiments}
\subsection{Experimental Setup}


\label{sec:experiments}

\section{Real-World Demonstration}

\section{Conclusion}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


\bibliographystyle{ieeetr}
\bibliography{references}






% \end{thebibliography}




\end{document}
