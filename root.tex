%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}

\title{\LARGE \bf
One Eye Leads the Blind: Asymmetric Multi-Robot Navigation with Visual Guidance
}


\author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
        University of Twente, 7500 AE Enschede, The Netherlands
        {\tt\small albert.author@papercept.net}}%
\thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In field operations where robots are exposed to extreme environments, the risk of sensor failure is a reality that affects the performance of robots and the ultimate success of their operation. This paper investigates the challenge of Asymmetric Multi-Robot Navigation, addressing a scenario where a sensing-rich "Leader" robot must guide a sensor-less "Follower" robot through an unknown environment.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Unmanned robotic systems are progressively being deployed in hazardous 
environments, including reconnaissance, search and rescue, and 
disaster response, where human presence is dangerous or not feasible~\cite{wang2025multi}. 
However, these missions often occur under harsh conditions that accelerate sensor degradation: dust and debris occlude cameras, electromagnetic interference disrupts lidar, and physical impacts could damage the perception hardware~\cite{ebadi2022present}. When a robot's sensors fail mid-mission in remote or inaccessible locations (e.g., underground tunnels, disaster sites, or enemy territory), the most likely response is to abort the mission and abandon the robot, resulting in costly equipment loss and incomplete objectives. This raises the question of whether a functional teammate can actively guide a sensor-impaired robot to safety or mission completion rather than abandoning it.

Asymmetric coordination strategies in multi-robot systems have been proposed to address mismatched capabilities among teammates. One approach uses a fixed sensor infrastructure that performs perception and localization on the robots' behalf, enabling effective navigation and coordination despite limited onboard sensing. This approach may not work in situations where setting up a fixed sensor infrastructure in the environment is not possible. For example, in dangerous environments that are being explored for the first time, it may be infeasible to set up such infrastructure.

Another approach is the leader–follower paradigm, in which a sensor-rich leader with privileged information computes safe trajectories while follower robots track and maintain formation along the leader’s path~\cite{petrinic2013leader}. However, this approach typically assumes that followers retain at least minimal sensing~\cite{wang2019vision} to track the leader or localize within the environment, an assumption that breaks down when a follower’s sensors have completely failed. This gap motivates the need for recovery mechanisms that remain effective even when a robot's perception sensor has failed.

Importantly, asymmetric navigation is not unique to robotic systems. For example, in complex, crowded, or unfamiliar environments, a blind person is often guided by a sighted companion who has privileged information about the environment, plans safe navigation trajectories, and communicates them through physical contact and voice, allowing safe movement while relying entirely on the guide. A similar idea appears in human–robot interaction, where robotic guide dogs with visual perception assist blind humans~\cite{hwang2025guidenav}. Inspired by these examples, we ask if a visually capable robot can guide another robot that has no perception at all, enabling safe navigation purely through its own sensing, localization and motion-based communication.

To address the challenges of asymmetric navigation under partial observability, we adopt a two-phase reinforcement learning framework based on privileged knowledge distillation. In Phase 1, we train a teacher policy in simulation using deep reinforcement learning (RL) with access to full ground-truth environment states, enabling the policy to learn robust navigation strategies with complete situational awareness. In Phase 2, we distill this expertise into a student policy that operates using only onboard sensing and limited observations. The student is trained via supervised imitation to replicate the teacher’s actions, thereby transferring privileged knowledge without requiring full state access at deployment. This simulation-to-reality pipeline enables strong generalization to unseen environments. 

We validate our approach through real-world deployment on two Hiwonder Orin robots, where a vision-less follower successfully navigates cluttered environments by executing movement commands communicated by a privileged leader.

The key contributions of this paper are as follows:

\begin{itemize}
    \item We introduce a novel asymmetric navigation framework in which a vision-enabled robot guides a vision-less robot through cluttered environments via communicated motion commands.
    
    \item We demonstrate successful real-world generalization through deployment on physical robots, validating robust navigation performance across diverse cluttered environments.

    
\end{itemize}


\section{RELATED WORKS}

Navigation of a sensorless agent via external sensing is validated by recent research demonstrating that autonomous navigation does not strictly require onboard exteroceptive sensors if external intelligence is available. (...Verma and Savkin) present a dynamic programming-based algorithm that enables a sensorless mobile robot to navigate dynamic environments using a fixed external sensor network. Their approach offloads localization, obstacle tracking, and path planning entirely to fixed infrastructure (LiDARs), proving that a robot can react to dynamic obstacles without onboard perception [1]. The methodology of transferring navigational competency from a knowledgeable agent to a sensor-limited agent is explored through "Teacher-Student" and "Teach-and-Repeat" paradigms. (Qu et al) utilize a two-stage training process for low-cost hexapods where a "Teacher" policy, trained with privileged information (height maps and joint feedback), distills knowledge into a "Student" policy that relies on limited onboard sensors.
Furthermore, the concept of asymmetric guidance is strongly represented in Human-Robot Interaction (HRI) for the visually impaired. Hwang et al [3]. developed "GuideNav," a robotic guide dog system that uses visual teach-and-repeat methods to guide blind humans. Their success in kilometer-scale guidance without LiDAR suggests that vision-based topological maps are a viable low-bandwidth mechanism for linking Leaders and Followers.
Robust Perception Capabilities for the Leader Agent For an asymmetric system to function, the "Leader" must possess high-fidelity robustness to compensate for the "Blind" followers. Recent literature establishes the necessary capabilities for such a leader.
Agility and Safety: He et al [4]. propose the "Agile But Safe" (ABS) framework, which employs a dual-policy setup (agile vs. recovery) governed by reach-avoid values. Hoeller et al [5] demonstrate that learning a belief state representation from depth images allows a robot to handle partial observability and dynamic obstacles. This memory-based representation is crucial for a Leader robot to predict environmental changes that might endanger the sensorless Follower. To ensure the Leader works in the "unknown environments", Yu et al.~\cite{yu2025depth} introduce domain adaptation techniques to align simulated depth features with real-world inputs. This ensures the Leader's perception remains reliable when transferring from training simulations to physical deployment [6].


\section{Problem Formulation}

Our goal 

\section{Methodology}

\section{Experiments and Results}

\section{Real-World Demonstration}

\section{Conclusion}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


\bibliographystyle{ieeetr}
\bibliography{references}
% \begin{thebibliography}{99}

% \bibitem{c1} S. C. Verma and A. V. Savkin, “Navigation of a sensorless autonomous mobile robot in unknown dynamic environments by an external sensor network,” in Proc. IEEE, 2025.
% \bibitem{c2} T. Qu, D. Li, A. Zakhor, W. Yu, and T. Zhang, “Versatile locomotion skills for hexapod robots,” arXiv preprint arXiv:2412.10628, 2024.
% \bibitem{c3} H. Hwang et al., “GuideNav: User-informed development of a vision-only robotic navigation assistant for blind travelers,” in Proc. ACM/IEEE Int. Conf. Human-Robot Interaction (HRI), Edinburgh, UK, 2026.

% \bibitem{c4} T. He, C. Zhang, W. Xiao, G. He, C. Liu, and G. Shi, “Agile but safe: Learning collision-free high-speed legged locomotion,” arXiv preprint arXiv:2401.17583, 2024.
% \bibitem{c5} D. Hoeller, L. Wellhausen, F. Farshidian, and M. Hutter, “Learning a state representation and navigation in cluttered and dynamic environments,” IEEE Robot. Autom. Lett., 2021.
% \bibitem{c6}  H. Yu, C. De Wagter, and G. C. H. E. de Croon, “Depth transfer: Learning to see like a simulator for real-world drone navigation,” arXiv preprint arXiv:2505.12428, 2025.
% \bibitem{c7} 
% \bibitem{c8} 
% \bibitem{c9} 
% \bibitem{c10}





% \end{thebibliography}




\end{document}
